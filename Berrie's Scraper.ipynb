{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0484a7ef",
   "metadata": {},
   "source": [
    "## Nederland"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b85aaa",
   "metadata": {},
   "source": [
    "### Aldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a45ddcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\"https://www.aldi.nl/zoeken.html?query=noten&searchCategory=Submitted%20Search&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&configure%5BclickAnalytics%5D=true\"\n",
    "        ,\"https://www.aldi.nl/producten/chips-noten/noten-zaden-en-pitten.html\"\n",
    "       ,\"https://www.aldi.nl/producten/chips-noten/zoutjes.html\"]\n",
    "\n",
    "# Create an empty list to store all product details\n",
    "all_products = []\n",
    "\n",
    "# Loop over the list of URLs\n",
    "for url in urls:\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the articles to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"mod-article-tile--default\")))\n",
    "\n",
    "    # Retrieve the elements after the wait\n",
    "    articles = driver.find_elements(By.CLASS_NAME, \"mod-article-tile--default\")\n",
    "\n",
    "    # Extract details for each article on the page\n",
    "    for article in articles:\n",
    "        # Use BeautifulSoup to parse the individual article's HTML\n",
    "        soup = BeautifulSoup(article.get_attribute('outerHTML'), \"html.parser\")\n",
    "\n",
    "        title = soup.find('span', class_='mod-article-tile__title').get_text(strip=True) if soup.find('span', class_='mod-article-tile__title') else 'Title not found'\n",
    "        promo_price_element = soup.find('s', class_='price__previous')\n",
    "        promo_price = promo_price_element.get_text(strip=True) if promo_price_element else 'Promo price not found'\n",
    "        current_price_element = soup.find('span', class_='price__wrapper')\n",
    "        current_price = current_price_element.get_text(strip=True) if current_price_element else 'Price not found'\n",
    "        weight = soup.find('span', class_='price__unit').get_text(strip=True) if soup.find('span', class_='price__unit') else 'Weight not found'\n",
    "\n",
    "        all_products.append((title, current_price, promo_price, weight))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD HH:MM:SS\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('Berrie.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write header only if the file is empty\n",
    "    if file.tell() == 0:\n",
    "        writer.writerow(['Title', 'Price', 'Promo Price', 'Weight', 'Timestamp'])  # CSV header\n",
    "    for product in all_products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(\"Data has been successfully saved\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8820526c",
   "metadata": {},
   "source": [
    "### Albert Heijn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3877d249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "import re  # Importing the regular expression module\n",
    "from datetime import datetime  # Importing datetime for timestamp\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "url = \"https://www.ah.nl/producten/chips-noten-toast-popcorn/noten?merk=AH&page=6\"\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# Borrowed the 'very cool :)' accepteer cookies button from TOTO scraper(group assignment)\n",
    "accept_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"decline-cookies\")))\n",
    "accept_button.click()\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# List to store the extracted product information\n",
    "products = []\n",
    "\n",
    "# Loop through all product articles\n",
    "for article in soup.find_all('article', class_='product-card-portrait_root__ZiRpZ'):\n",
    "    # Extract the price from the aria-label of the sr-only span\n",
    "    price_span = article.find('span', class_='sr-only')\n",
    "    if price_span:\n",
    "        # Use regular expression to extract the numeric price (e.g., 1.99)\n",
    "        match = re.search(r'[\\d]+[.,][\\d]+', price_span.get('aria-label'))\n",
    "        price = match.group() if match else 'Price not found'\n",
    "    else:\n",
    "        price = 'Price not found'\n",
    "        \n",
    "    # Extract the promo price (if available) from the correct div\n",
    "    promo_price_span = article.find('div', class_='price-amount_highlight__ekL92')\n",
    "    if promo_price_span:\n",
    "        # Use a nested find to get the sr-only span within the promo price div\n",
    "        promo_price_span_inner = promo_price_span.find('span', class_='sr-only')\n",
    "        if promo_price_span_inner:\n",
    "            # Use regular expression to extract the numeric promo price (e.g., 6.53)\n",
    "            match_promo_price = re.search(r'[\\d]+[.,][\\d]+', promo_price_span_inner.get('aria-label'))\n",
    "            promo_price = match_promo_price.group() if match_promo_price else 'Promo price not found'\n",
    "        else:\n",
    "            promo_price = 'Promo price not found'\n",
    "    else:\n",
    "        promo_price = 'Promo price not found'\n",
    "\n",
    "    # Extract the product title from the title attribute of the anchor tag\n",
    "    title_tag = article.find('a', class_='link_root__EqRHd')\n",
    "    title = title_tag.get('title') if title_tag else 'Title not found'\n",
    "    \n",
    "    # Extract the weight from the product-unit-size span\n",
    "    weight_span = article.find('span', class_='price_unitSize__Hk6E4')\n",
    "    weight = weight_span.get_text(strip=True) if weight_span else 'Weight not found'\n",
    "\n",
    "    # Store the extracted information as a tuple, including promo price\n",
    "    products.append((title, price, promo_price, weight))\n",
    "    \n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD HH:MM:SS\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('AH_Berrie.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for product in products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(f\"Data has been successfully saved\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5683471e",
   "metadata": {},
   "source": [
    "## Duitsland"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b25a7d",
   "metadata": {},
   "source": [
    "### Aldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2405a971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\"https://www.aldi-nord.de/suchergebnisse.html?query=asiatisce%20snack&searchCategory=Submitted%20Search&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\"\n",
    "       ,\"https://www.aldi-nord.de/suchergebnisse.html?query=kerne&searchCategory=Submitted%20Search&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_de_de_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\"]\n",
    "# Create an empty list to store all product details\n",
    "all_products = []\n",
    "\n",
    "# Loop over the list of URLs\n",
    "for url in urls:\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the articles to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"mod-article-tile--default\")))\n",
    "\n",
    "    # Retrieve the elements after the wait\n",
    "    articles = driver.find_elements(By.CLASS_NAME, \"mod-article-tile--default\")\n",
    "\n",
    "    # Extract details for each article on the page\n",
    "    for article in articles:\n",
    "        # Use BeautifulSoup to parse the individual article's HTML\n",
    "        soup = BeautifulSoup(article.get_attribute('outerHTML'), \"html.parser\")\n",
    "\n",
    "        title = soup.find('span', class_='mod-article-tile__title').get_text(strip=True) if soup.find('span', class_='mod-article-tile__title') else 'Title not found'\n",
    "        promo_price_element = soup.find('s', class_='price__previous')\n",
    "        promo_price = promo_price_element.get_text(strip=True) if promo_price_element else 'Promo price not found'\n",
    "        current_price_element = soup.find('span', class_='price__wrapper')\n",
    "        current_price = current_price_element.get_text(strip=True) if current_price_element else 'Price not found'\n",
    "        weight = soup.find('span', class_='price__unit').get_text(strip=True) if soup.find('span', class_='price__unit') else 'Weight not found'\n",
    "\n",
    "        all_products.append((title, current_price, promo_price, weight))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD HH:MM:SS\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('Berrie.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write header only if the file is empty\n",
    "    if file.tell() == 0:\n",
    "        writer.writerow(['Title', 'Price', 'Promo Price', 'Weight', 'Timestamp'])  # CSV header\n",
    "    for product in all_products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(\"Data has been successfully saved\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be98f2de",
   "metadata": {},
   "source": [
    "### Globus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16487966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Pages: 4\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Data has been written to berrie.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime  # Importing datetime module\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "# Base URL for pagination\n",
    "base_url = \"https://produkte.globus.de/bedburg/suesses-salziges/nuesse-fruechte/?max-price=6.89&min-price=1.19&p=\"\n",
    "\n",
    "# Open the URL (first page)\n",
    "driver.get(base_url + \"1\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Wait for the page to load and extract the number of pages\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "pagination_element = soup.find(\"li\", class_=\"page-item page-last\")\n",
    "total_pages = int(pagination_element.find(\"input\")[\"value\"]) if pagination_element else 1  # Default to 1 page if not found\n",
    "\n",
    "print(f\"Total Pages: {total_pages}\")\n",
    "\n",
    "# Get the current timestamp for CSV file\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open('Berrie.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Product Title\", \"Price\", \"Weight\", \"Promo Price\", \"Timestamp\"])  # Write header row\n",
    "    \n",
    "    # Loop over each page\n",
    "    for page_number in range(1, total_pages + 1):\n",
    "        print(f\"Scraping page {page_number}...\")\n",
    "        \n",
    "        # Get the URL for the current page\n",
    "        page_url = base_url + str(page_number)\n",
    "        driver.get(page_url)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Get the page source and parse it with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "        # Loop through all product cards and extract data\n",
    "        for product_card in soup.find_all(\"div\", class_=\"product-info\"):\n",
    "            # Extract product title\n",
    "            title_tag = product_card.find(\"a\", class_=\"product-image-link product-name\")\n",
    "            title = title_tag.get(\"title\").strip() if title_tag else \"Title not found\"\n",
    "\n",
    "            # Extract price\n",
    "            price_div = product_card.find(\"div\", class_=\"unit-price js-unit-price\")\n",
    "            price = price_div.get(\"data-value\") if price_div and price_div.has_attr(\"data-value\") else \"Price not found\"\n",
    "\n",
    "            # Extract weight\n",
    "            weight_div = product_card.find(\"div\", class_=\"price-unit-content\")\n",
    "            weight = weight_div.text.strip() if weight_div else \"Weight not found\"\n",
    "            \n",
    "            # Extract promo price\n",
    "            promo_price = \"Promo price not found\"  # Default value in case promo price is not found\n",
    "            promo_price_div = product_card.find(\"div\", class_=\"product-price-globus-discount\")\n",
    "            if promo_price_div:\n",
    "                promo_price_element = promo_price_div.find(\"div\", class_=\"unit-price js-unit-price discount-price\")\n",
    "                if promo_price_element:\n",
    "                    promo_price = promo_price_element.text.strip()\n",
    "\n",
    "            # Write the product data to the CSV file\n",
    "            writer.writerow([title, price, promo_price, weight, timestamp])  # Adding timestamp to the row\n",
    "\n",
    "# Close the driver after extracting data\n",
    "driver.quit()\n",
    "\n",
    "print(\"Data has been written to berrie.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0f9f2",
   "metadata": {},
   "source": [
    "## Frankrijk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7e262",
   "metadata": {},
   "source": [
    "### Aldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a308e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\"https://www.aldi.fr/produits/epicerie-salee/biscuit-aperitif-chips.html\"\n",
    "        ,\"https://www.aldi.fr/recherche.html?query=trader%20joe&searchCategory=Submitted%20Search\"]\n",
    "\n",
    "        # Create an empty list to store all product details\n",
    "all_products = []\n",
    "\n",
    "# Loop over the list of URLs\n",
    "for url in urls:\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the articles to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"mod-article-tile--default\")))\n",
    "\n",
    "    # Retrieve the elements after the wait\n",
    "    articles = driver.find_elements(By.CLASS_NAME, \"mod-article-tile--default\")\n",
    "\n",
    "    # Extract details for each article on the page\n",
    "    for article in articles:\n",
    "        # Use BeautifulSoup to parse the individual article's HTML\n",
    "        soup = BeautifulSoup(article.get_attribute('outerHTML'), \"html.parser\")\n",
    "\n",
    "        title = soup.find('span', class_='mod-article-tile__title').get_text(strip=True) if soup.find('span', class_='mod-article-tile__title') else 'Title not found'\n",
    "        promo_price_element = soup.find('s', class_='price__previous')\n",
    "        promo_price = promo_price_element.get_text(strip=True) if promo_price_element else 'Promo price not found'\n",
    "        current_price_element = soup.find('span', class_='price__wrapper')\n",
    "        current_price = current_price_element.get_text(strip=True) if current_price_element else 'Price not found'\n",
    "        weight = soup.find('span', class_='price__unit').get_text(strip=True) if soup.find('span', class_='price__unit') else 'Weight not found'\n",
    "\n",
    "        all_products.append((title, current_price, promo_price, weight))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD HH:MM:SS\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('Berrie.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for product in all_products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(\"Data has been successfully saved\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287ca96a",
   "metadata": {},
   "source": [
    "### Carrefour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ddda758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "options = Options()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Open the URL\n",
    "url = \"https://www.carrefour.fr/s?filters%5Bfacet_marque%5D%5B0%5D=CARREFOUR&q=melange&noRedirect=1&userIsPro=0&page=1\"\n",
    "driver.get(url)\n",
    "time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "# Wait for the new button to be clickable\n",
    "param_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"onetrust-pc-btn-handler\")))\n",
    "param_button.click()\n",
    "\n",
    "# Wait for and click the \"refuser\" button\n",
    "confirm_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"ot-pc-refuse-all-handler\")))\n",
    "confirm_button.click()\n",
    "\n",
    "# Parse page source with BeautifulSoup\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# List to store product information\n",
    "products = []\n",
    "\n",
    "# Extract product details\n",
    "for product_pod in soup.find_all(\"div\", class_=\"main-layout__info-zone\"):\n",
    "    # Extract title\n",
    "    title_tag = product_pod.find(\"a\", class_=\"product-card-title\")\n",
    "    title = title_tag.text.strip() if title_tag else \"Title not found\"\n",
    "\n",
    "    # Extract weight\n",
    "    weight_tag = product_pod.find(\"p\", class_=\"pl-text--size-m\")\n",
    "    weight = weight_tag.text.strip() if weight_tag else \"Weight not found\"\n",
    "\n",
    "    # Extract current price (main price)\n",
    "    price_main_tag = product_pod.find(\"div\", class_=\"product-price__amount--main\")\n",
    "    if price_main_tag:\n",
    "        price_main_parts = price_main_tag.find_all(\"p\", class_=\"product-price__content\")\n",
    "        if len(price_main_parts) >= 2:\n",
    "            current_price = f\"{price_main_parts[0].text.strip()}{price_main_parts[1].text.strip()} €\"\n",
    "        else:\n",
    "            current_price = \"Price not found\"\n",
    "    else:\n",
    "        current_price = \"Price not found\"\n",
    "\n",
    "    # Extract promotional price\n",
    "    promo_price_tag = product_pod.find(\"div\", class_=\"product-price__amount--old\")\n",
    "    if promo_price_tag:\n",
    "        promo_price_parts = promo_price_tag.find_all(\"p\", class_=\"product-price__content\")\n",
    "        if len(promo_price_parts) >= 2:\n",
    "            promo_price = f\"{promo_price_parts[0].text.strip()},{promo_price_parts[1].text.strip()} €\"\n",
    "        else:\n",
    "            promo_price = \"Promo price not found\"\n",
    "    else:\n",
    "        promo_price = \"Promo price not found\"\n",
    "\n",
    "    # Append extracted information to the list\n",
    "    products.append((title, current_price, promo_price, weight))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD HH:MM:SS\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('Berrie.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for product in products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(\"Data has been successfully saved\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7dc72",
   "metadata": {},
   "source": [
    "## Polen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2a03e",
   "metadata": {},
   "source": [
    "### Aldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45e7dad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\"https://www.aldi.pl/szukaj.html?query=orzechy%20trader&searchCategory=Suggested%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12\"\n",
    "        ,\"https://www.aldi.pl/szukaj.html?query=asia&searchCategory=Suggested%20Search&configure%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_pl_pl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12\"]\n",
    "# Create an empty list to store all product details\n",
    "all_products = []\n",
    "\n",
    "# Loop over the list of URLs\n",
    "for url in urls:\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the articles to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"mod-article-tile--default\")))\n",
    "\n",
    "    # Retrieve the elements after the wait\n",
    "    articles = driver.find_elements(By.CLASS_NAME, \"mod-article-tile--default\")\n",
    "\n",
    "    # Extract details for each article on the page\n",
    "    for article in articles:\n",
    "        # Use BeautifulSoup to parse the individual article's HTML\n",
    "        soup = BeautifulSoup(article.get_attribute('outerHTML'), \"html.parser\")\n",
    "\n",
    "        title = soup.find('span', class_='mod-article-tile__title').get_text(strip=True) if soup.find('span', class_='mod-article-tile__title') else 'Title not found'\n",
    "        promo_price_element = soup.find('s', class_='price__previous')\n",
    "        promo_price = promo_price_element.get_text(strip=True) if promo_price_element else 'Promo price not found'\n",
    "        current_price_element = soup.find('span', class_='price__wrapper')\n",
    "        current_price = current_price_element.get_text(strip=True) if current_price_element else 'Price not found'\n",
    "        weight = soup.find('span', class_='price__unit').get_text(strip=True) if soup.find('span', class_='price__unit') else 'Weight not found'\n",
    "\n",
    "        all_products.append((title, current_price, promo_price, weight))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD HH:MM:SS\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('Berrie.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write header only if the file is empty\n",
    "    if file.tell() == 0:\n",
    "        writer.writerow(['Title', 'Price', 'Promo Price', 'Weight', 'Timestamp'])  # CSV header\n",
    "    for product in all_products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(\"Data has been successfully saved\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc973ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7e1b9d5",
   "metadata": {},
   "source": [
    "### Bydronka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "391fbd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie consent not found for URL: https://zakupy.biedronka.pl/artykuly-spozywcze/przekaski/bakalie/ or took too long to load\n",
      "Data has been successfully saved\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "options = Options()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://zakupy.biedronka.pl/artykuly-spozywcze/przekaski/orzeszki/\",\n",
    "    \"https://zakupy.biedronka.pl/artykuly-spozywcze/przekaski/bakalie/\"\n",
    "]\n",
    "\n",
    "# List to store all product information across multiple pages\n",
    "all_products = []\n",
    "\n",
    "# Loop over each URL\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "    try:\n",
    "        # Wait for the cookie consent button to be clickable (increased timeout)\n",
    "        param_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.ID, \"onetrust-pc-btn-handler\")))\n",
    "        param_button.click()\n",
    "\n",
    "        # Wait for and click the button to confirm cookie consent\n",
    "        confirm_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.CLASS_NAME, \"ot-pc-refuse-all-handler\")))\n",
    "        confirm_button.click()\n",
    "    except TimeoutException:\n",
    "        print(f\"Cookie consent not found for URL: {url} or took too long to load\")\n",
    "\n",
    "    # Parse page source with BeautifulSoup\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Extract product information for the current page\n",
    "    for product_pod in soup.find_all(\"div\", class_=\"product-tile js-product-tile\"):\n",
    "        # Extract title\n",
    "        title_tag = product_pod.find(\"div\", class_=\"product-tile__name product-tile__name--overflow\")\n",
    "        title = title_tag.text.strip() if title_tag else \"Title not found\"\n",
    "\n",
    "        # Extract weight (only the weight value, e.g., \"0.2kg\")\n",
    "        weight_tag = product_pod.find(\"div\", class_=\"packaging-details\")\n",
    "        if weight_tag:\n",
    "            weight = weight_tag.contents[0].strip()  # Get the first part before the <span> tag\n",
    "        else:\n",
    "            weight = \"Weight not found\"\n",
    "        \n",
    "        # Extract current price (main price)\n",
    "        price_main_tag = product_pod.find(\"div\", class_=\"price-tile__sales\")\n",
    "        if price_main_tag:\n",
    "            # Extract the integer part of the price\n",
    "            integer_part = price_main_tag.find(text=True, recursive=False).strip() if price_main_tag else None\n",
    "            decimal_part = price_main_tag.find(\"span\", class_=\"price-tile__decimal\")\n",
    "            if integer_part and decimal_part:\n",
    "                # Combine integer and decimal parts into one properly formatted price\n",
    "                raw_price = f\"{integer_part.strip()}{decimal_part.text.strip()}\"  # Combine without formatting\n",
    "                if len(raw_price) > 2:\n",
    "                    current_price = f\"{raw_price[:-2]}.{raw_price[-2:]} zł\"  # Insert decimal point two digits from the end\n",
    "                else:\n",
    "                    current_price = f\"0.{raw_price} zł\"  # Handle cases where price is less than 1 zł\n",
    "            else:\n",
    "                current_price = \"Price not found\"\n",
    "        else:\n",
    "            current_price = \"Price not found\"\n",
    "\n",
    "        # Remove any extra spaces (just in case)\n",
    "        current_price = current_price.replace(\" \", \"\").strip()\n",
    "\n",
    "        # Extract promo price if available\n",
    "        promo_price_tag = product_pod.find(\"div\", class_=\"product-tile-prices__regular\")\n",
    "        if promo_price_tag:\n",
    "            promo_price = promo_price_tag.find(\"span\", class_=\"product-tile-prices__amount\")\n",
    "            if promo_price:\n",
    "                promo_price = promo_price.text.strip()\n",
    "            else:\n",
    "                promo_price = \"Promo Price not found\"\n",
    "        else:\n",
    "            promo_price = \"Promo Price not found\"\n",
    "\n",
    "        # Append extracted information to the list\n",
    "        all_products.append((title, current_price, promo_price, weight))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('Berrie.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for product in all_products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(\"Data has been successfully saved\")\n",
    "\n",
    "# Quit the driver\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
